# -*- coding: utf-8 -*-
"""bcp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uFiWh3gS3lxx_H9oGQiIjI3-3f65HPs1

**installing libraries**
"""

!pip install pandas
!pip install scikit-learn
!pip install matplotlib
!pip install numpy
!pip install seaborn!
!pip install tensorflow
!pip install keras

"""DATA COLLECTION**"""

import pandas as pd
import numpy as np

data=pd.read_csv("/data.csv")
data.head(5)
data.tail()

"""EXPLORING DATA"""

data.info()

data.diagnosis.value_counts()

data.isna().sum()

"""**VISUALIZATION**"""

import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go

sns.set_style('darkgrid')
plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
plt.hist( data.diagnosis)
plt.title("Counts of Diagnosis")
plt.xlabel("Diagnosis")

"""# New Section"""

plt.subplot(1, 2, 1)

sns.countplot(x='diagnosis', data=data);

cols = ["diagnosis", "radius_mean", "texture_mean", "perimeter_mean", "area_mean"]

sns.pairplot(data[cols], hue="diagnosis")
plt.show()

import matplotlib.pyplot as plt
import numpy as np

size = len(data['texture_mean'])

area = np.pi * (15 * np.random.rand( size ))**2
colors = np.random.rand( size )

plt.xlabel("texture mean")
plt.ylabel("radius mean")
plt.scatter(data['texture_mean'], data['radius_mean'], s=area, c=colors, alpha=0.5);

"""DATA **FILTERING**"""

from sklearn.preprocessing import LabelEncoder
data.head(2)

print(data.diagnosis.value_counts())
print("\n", data.diagnosis.value_counts().sum())

from sklearn.preprocessing import LabelEncoder


label_encoder = LabelEncoder()


data['diagnosis_encoded'] = label_encoder.fit_transform(data['diagnosis'])

cols = ['diagnosis_encoded', 'radius_mean', 'texture_mean', 'perimeter_mean',
       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',
       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']

print(len(cols))
data[cols].corr()

plt.figure(figsize=(12, 9))

plt.title("Correlation Graph")

cmap = sns.diverging_palette( 1000, 120, as_cmap=True)
sns.heatmap(data[cols].corr(), annot=True, fmt='.1%',  linewidths=.05, cmap=cmap)

plt.figure(figsize=(15, 10))


fig = px.imshow(data[cols].corr());
fig.show()

"""MODEL IMPLEMENTAION"""

from sklearn.model_selection import train_test_split

from sklearn.preprocessing import StandardScaler

from sklearn.linear_model import LogisticRegression

from sklearn.tree import DecisionTreeClassifier

from sklearn.ensemble import RandomForestClassifier

from sklearn.naive_bayes import GaussianNB

from sklearn.neighbors import KNeighborsClassifier

from sklearn.metrics import accuracy_score, confusion_matrix, f1_score

from sklearn.metrics import classification_report

from sklearn.model_selection import KFold

from sklearn.model_selection import cross_validate, cross_val_score

from sklearn.svm import SVC

from sklearn import metrics

data.columns

prediction_feature = [ "radius_mean",  'perimeter_mean', 'area_mean', 'symmetry_mean', 'compactness_mean', 'concave points_mean']

targeted_feature = 'diagnosis'

len(prediction_feature)
X = data[prediction_feature]
print(X)
y = data.diagnosis
y

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=15)

print(X_train)

sc = StandardScaler()

X_train = sc.fit_transform(X_train)
X_test = sc.fit_transform(X_test)

"""MODEL PREDICTION"""

def model_building(model, X_train, X_test, y_train, y_test):



    model.fit(X_train, y_train)
    score = model.score(X_train, y_train)
    predictions = model.predict(X_test)
    accuracy = accuracy_score(predictions, y_test)

    return (score, accuracy, predictions)
# Moved the models_list variable outside the function to make it globally accessible
models_list = {
    "LogisticRegression" :  LogisticRegression(),
    "RandomForestClassifier" :  RandomForestClassifier(n_estimators=10, criterion='entropy', random_state=5),
    "DecisionTreeClassifier" :  DecisionTreeClassifier(criterion='entropy', random_state=0),
    "SVC" :  SVC(),
}

print(list(models_list.keys()))
print(list(models_list.values()))

def cm_metrix_graph(cm):

    sns.heatmap(cm,annot=True,fmt="d")
    plt.show()

df_prediction = []
confusion_matrixs = []
df_prediction_cols = [ 'model_name', 'score', 'accuracy_score' , "accuracy_percentage"]

for name, model in zip(list(models_list.keys()), list(models_list.values())):

    (score, accuracy, predictions) = model_building(model, X_train, X_test, y_train, y_test )

    print("\n\nClassification Report of '"+ str(name), "'\n")

    print(classification_report(y_test, predictions))

    df_prediction.append([name, score, accuracy, "{0:.2%}".format(accuracy)])

    # For Showing Metrics
    confusion_matrixs.append(confusion_matrix(y_test, predictions))


df_pred = pd.DataFrame(df_prediction, columns=df_prediction_cols)

print(len(confusion_matrixs))

plt.figure(figsize=(10, 2))

for index, cm in enumerate(confusion_matrixs):

    # Removed 'up' as it seems to serve no purpose

    cm_metrix_graph(cm)
    plt.tight_layout(pad=True)

df_pred

df_pred.sort_values('score', ascending=False)

from  sklearn.model_selection import GridSearchCV

"""HYPERTUNING THE MODEL
DECISION TREE CLASSSIFIER:
"""

model = DecisionTreeClassifier()


param_grid = {'max_features': ['auto', 'sqrt', 'log2'],
              'min_samples_split': [2,3,4,5,6,7,8,9,10],
              'min_samples_leaf':[2,3,4,5,6,7,8,9,10] }


gsc = GridSearchCV(model, param_grid, cv=10)

gsc.fit(X_train, y_train)

print("\n Best Score is ")
print(gsc.best_score_)

print("\n Best Estinator is ")
print(gsc.best_estimator_)

print("\n Best Parametes are")
print(gsc.best_params_)

"""K-NEIGHBOUR CLASSIFICATION:"""

model = KNeighborsClassifier()


# Tunning Params
param_grid = {
    'n_neighbors': list(range(1, 30)),
    'leaf_size': list(range(1,30)),
    'weights': [ 'distance', 'uniform' ]
}


# Implement GridSearchCV
gsc = GridSearchCV(model, param_grid, cv=10)

# Model Fitting
gsc.fit(X_train, y_train)

print("\n Best Score is ")
print(gsc.best_score_)

print("\n Best Estimator is ")
print(gsc.best_estimator_) # Print the best estimator

"""SVM:"""

model = SVC()


# Tunning Params
param_grid = [
              {'C': [1, 10, 100, 1000],
               'kernel': ['linear']
              },
              {'C': [1, 10, 100, 1000],
               'gamma': [0.001, 0.0001],
               'kernel': ['rbf']
              }
]


# Implement GridSearchCV
gsc = GridSearchCV(model, param_grid, cv=10) # 10 Cross Validation

# Model Fitting
gsc.fit(X_train, y_train)

print("\n Best Score is ")
print(gsc.best_score_)

print("\n Best Estinator is ")
print(gsc.best_estimator_)

print("\n Best Parametes are")
print(gsc.best_params_)

"""DEPLOY MODEL"""

import pickle as pkl
model.fit(X_train, y_train)

filename = 'finalized_model.sav'
pkl.dump(model, open(filename, 'wb'))


loaded_model = pkl.load(open(filename, 'rb'))
result = loaded_model.score(X_test, y_test)
print(result)

logistic_model = LogisticRegression()
logistic_model.fit(X_train, y_train)

filename = 'logistic_model.pkl'
pkl.dump(logistic_model, open(filename, 'wb'))
loaded_model = pkl.load(open(filename, 'rb')) # rb means read as binary
result = loaded_model.score(X_test, y_test)